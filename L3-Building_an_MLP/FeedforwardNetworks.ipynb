{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#######################\n",
    "#   Helper functions  #\n",
    "#######################\n",
    "# Linear activation\n",
    "def a(x,w,b):\n",
    "    a_out = x.dot(w) + b\n",
    "    return a_out\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    s = 1/(1+np.exp(-z))\n",
    "    return s\n",
    "\n",
    "# Logistic unit\n",
    "def logistic(x,w,b):\n",
    "    s = sigmoid(a(x,w,b))\n",
    "    y = np.round(s)\n",
    "    return np.array([y,s]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's code up a 2-layer MLP. Our network will take in 2-dimensional input, will have a single hidden layer of 3 units, and will have a single output classification.\n",
    "\n",
    "We'll create randomized initial weights for our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "#Setting up dimensions of 2 Layer NN\n",
    "\n",
    "n_dims = 2\n",
    "n_hidden_units = 3\n",
    "\n",
    "# Settng up the weight parameters for Layer 1\n",
    "w_11, w_12, w_13, w_21, w_22, w_23 = np.random.random(n_dims * n_hidden_units)\n",
    "\n",
    "# Setting up weight parameters for Layer 2\n",
    "v_1,v_2,v_3 = np.random.random(n_hidden_units)\n",
    "\n",
    "# Random intializiation of the biases\n",
    "# Layer 1\n",
    "b_11,b_12,b_13 = np.random.random(n_hidden_units)\n",
    "b_1 = np.array([b_11,b_12,b_13])\n",
    "# Layer 2\n",
    "b_2 = np.random.random(1)\n",
    "\n",
    "# Restructing for ease of implementation\n",
    "w_1 = np.array([w_11,w_12,w_13])\n",
    "w_2 = np.array([w_21,w_22, w_23])\n",
    "#w_3 = np.array([w_31,w_32])\n",
    "\n",
    "w = np.array([w_1,w_2])\n",
    "v = np.array([v_1,v_2,v_3])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.69014075,  0.4193662 ,  0.85073887],\n",
       "       [ 0.48604109,  0.03893225,  0.48501369]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feedforward_network_v1(x, w, v, b_1, b_2):\n",
    "    '''\n",
    "    A simple 2 layer neural network with sigmoid activation and binary output.\n",
    "    '''\n",
    "    # Setting up our output y\n",
    "    num_rows,num_columns = x.shape\n",
    "    y = np.zeros((num_rows))\n",
    "    \n",
    "    for i in range(num_rows):\n",
    "        x_i = x[i,:]\n",
    "        \n",
    "        # Linear activations into hidden units\n",
    "        a1 = x_i[0]*w[0,0] + x_i[1]*w[1,0] + b_1[0]\n",
    "        a2 = x_i[0]*w[0,1] + x_i[1]*w[1,1] + b_1[1]\n",
    "        a3 = x_i[0]*w[0,2] + x_i[1]*w[1,2] + b_1[2]\n",
    "        \n",
    "        # output of hidden units\n",
    "        h_1 = sigmoid(a1)\n",
    "        h_2 = sigmoid(a2)\n",
    "        h_3 = sigmoid(a3)\n",
    "        h = np.array([h_1,h_2,h_3])\n",
    "        \n",
    "        # Output of network\n",
    "        y[i] = sigmoid(h_1*v[0] + h_2*v[1] + h_3*v[2] + b_2)\n",
    "    \n",
    "    return np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# here's some randomized input data\n",
    "num_samples=50\n",
    "x = np.random.uniform(low=(-5.0), high=5.0, size=2*num_samples).reshape(num_samples,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.93562321,  0.94707351,  0.90039579,  0.78074813,  0.87728824])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feedforward_network_v1(x[:5,:], w, v, b_1, b_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all of this arithmetic was done explicity by indexing into our weight matrixes and vectors. But we know we can accomplish the same thing with vector arithmetic. For the linear activations into a hidden unit, this weighted sum is the same as a dot product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feedforward_network_v2(x, w, v, b_1, b_2):\n",
    "    '''\n",
    "    A simple 2 layer neural network with sigmoid activation and binary output.\n",
    "    '''\n",
    "    # Setting up our output y\n",
    "    l,_ = x.shape\n",
    "    y = np.zeros((l))\n",
    "    \n",
    "    for i in range(l):\n",
    "        x_i = x[i,:]\n",
    "        # Setting up the hidden units\n",
    "        h_1 = sigmoid(a(x[i,:],w[:,0],b_1[0]))\n",
    "        h_2 = sigmoid(a(x[i,:],w[:,1],b_1[1]))\n",
    "        h_3 = sigmoid(a(x[i,:],w[:,2],b_1[2]))\n",
    "        h = np.array([h_1,h_2,h_3])\n",
    "        # Calculating the output\n",
    "        y[i] = sigmoid(a(h,v,b_2))\n",
    "    \n",
    "    return np.array(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.93562321,  0.94707351,  0.90039579,  0.78074813,  0.87728824])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feedforward_network_v2(x[:5,:], w, v, b_1, b_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, note that we're using a for-loop to compute the forward pass for each input X. This can also be vectorized instead of using a loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feedforward_network_v3(x, w, v, b_1, b_2):\n",
    "    '''\n",
    "    A simple 2 layer neural network with sigmoid activation and binary output.\n",
    "    '''\n",
    "    \n",
    "    a = np.dot(x,w) + b_1\n",
    "    h = sigmoid(a)\n",
    "    y = sigmoid(np.dot(h,v) + b_2)\n",
    "\n",
    "    return np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.93562321,  0.94707351,  0.90039579,  0.78074813,  0.87728824])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feedforward_network_v3(x[:5,:], w, v, b_1, b_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll pick a simple 1D regression example: we are representing arbitrary functions y of x. So we have a scalar input and a scalar output. Let's explore what kinds of functions our neural net is able to produce.\n",
    "\n",
    "By drawing random weights, we'll visualize a particular \"setting\" of a neural net, to see what kind of functions are possible. \n",
    "\n",
    "Depending on the number of hidden nodes, and the strength of the weights, we'll find that we can represent some pretty complex functions with a neural net. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomize_weights(num_hidden_nodes, input_dim=1, wt_scale=1):\n",
    "    w = wt_scale*np.random.randn(num_hidden_nodes*input_dim).reshape((input_dim, num_hidden_nodes))\n",
    "    v = wt_scale*np.random.randn(num_hidden_nodes)\n",
    "    h_bias = wt_scale*np.random.randn(num_hidden_nodes)\n",
    "    y_bias = wt_scale*np.random.randn(1) \n",
    "    return (w, v, h_bias, y_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nklQgw12SCmS4S1KBDHdJKtD/B4KpB5AEtBoEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1109c1c10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x=np.expand_dims(np.linspace(-20,20,250), 1)\n",
    "z = randomize_weights(3,wt_scale=1)\n",
    "y_pred = feedforward_network_v3(x, *z)\n",
    "plt.plot(x, y_pred)\n",
    "plt.ylim((0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Code up the XOR network from Chapter 6 of the book. Use the weights provided on page 174 to compute the forward pass. Use a sigmoid for the activation function. What is the predicted output for each of the 4 observations of the XOR dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Our representation of these ANN transformations can have an impact on computational efficiency. As you might guess, when we leverage matrix and vector arithmetic, our feedforward computations are much faster.\n",
    "\n",
    "Code up a simple multilayer network of your choice. Implement the feedfoward transformations with vector operations and then with explicit for-loop operations. Time how long it takes in each case to compute a forward pass for a set of input datapoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise (Optional) - Deep Networks\n",
    "\n",
    "In our 2-layer MLP, the hidden layer was a little bit specialized because its inputs were datapoints X and its output went into predicting targets Y. But none of that *has* to be the case. In a more general form, a hidden layer's input could be the outputs of another hidden layer. And a hidden layer's outputs could go on to be the inputs for another hidden layer. This framework is a **deep neural network**. \n",
    "\n",
    "Extend our prevoius 2-layer MLP code into a 3-layer MLP. \n",
    "\n",
    "\n",
    "Use the same number of hidden nodes in both layers (3 is fine).\n",
    "Think about what additional weights you'll need: \n",
    " - another weight matrix? \n",
    " - another bias vector? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
