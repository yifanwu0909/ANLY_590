{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joshuah/venv3/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# ---- Programming backprop from scratch -----\n",
    "##########################\n",
    "#   Importing libraries  #\n",
    "##########################\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "#   Activation Functions  #\n",
    "###########################\n",
    "ReLU = np.vectorize(lambda z: np.fmax(0,z))\n",
    "sigmoid = lambda z: 1 / (1 + np.exp(-z))\n",
    "softmax = lambda z: np.exp(z)/(np.sum(np.exp(z),axis=1))[:,np.newaxis]\n",
    "tanh = lambda z: np.tanh(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#######################\n",
    "#  Utility Functions  #\n",
    "#######################\n",
    "def predict(Y_hat):\n",
    "    return np.argmax(Y_hat, axis=1)\n",
    "\n",
    "def error_rate(Y_hat, cl):\n",
    "    prediction = predict(Y_hat)\n",
    "    return np.mean(prediction != cl)\n",
    "\n",
    "def cost(Y_hat, Y):\n",
    "    tot = Y * np.log(Y_hat)\n",
    "    return -tot.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################################\n",
    "#   1- Hidden Layer ReLU Network   #\n",
    "####################################\n",
    "def forward(X,parameters):\n",
    "    # Unpacking parameters    \n",
    "    W,b1,V,b2 = parameters\n",
    "    \n",
    "    # Forward pass\n",
    "    a1 = X.dot(W) + b1\n",
    "    H = sigmoid(a1)\n",
    "    a2 = H.dot(V) + b2\n",
    "    Y_hat = softmax(a2)\n",
    "    return H,Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################################\n",
    "#   2 - Hidden Layer ReLU Network   #\n",
    "####################################\n",
    "def forward(X,parameters):\n",
    "    # Unpacking parameters    \n",
    "    W1,b1,W2,b2,V,v3 = parameters\n",
    "    \n",
    "    # Forward pass\n",
    "    a1 = X.dot(W1) + b1\n",
    "    H1 = sigmoid(a1)\n",
    "    a2 = H.dot(W2) + b2\n",
    "    H2 = sigmoid(a2)\n",
    "    a3 = H2.dot(V) + b3\n",
    "    Y_hat = softmax(a3)\n",
    "    return H1,H2,Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################\n",
    "#   Gradient   #\n",
    "################\n",
    "\n",
    "def grad(X,H,Y,Y_hat,parameters):  \n",
    "    # Unpacking parameters    \n",
    "    W,b1,V,b2 = parameters\n",
    "  \n",
    "    # Gradients - sigmoid\n",
    "    dW = X.T.dot((Y_hat-Y).dot(V) * (H * (1 - H)))\n",
    "    db1 = (Y_hat-Y).dot(V) * (H * (1 - H)).sum(axis=0)\n",
    "    dV = H.T.dot(Y_hat - Y)\n",
    "    db2 = (Y_hat - Y).sum(axis=0)\n",
    "    \n",
    "    #return {'dV':dV,'db2':db2,'dW':dW,'db1':db1}\n",
    "    return dW,db1,dV,db2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#   Parameter Update: Momentum + Regularization   #\n",
    "###################################################\n",
    "def parameter_update(parameters,  grads, \n",
    "                     momentum_params = [0,0,0,0], \n",
    "                     lr = 1, reg = 0, alpha = 0):\n",
    "    # Unpacking parameters            \n",
    "    W,b1,V,b2 = parameters\n",
    "    dW,db1,dV,db2 = grads\n",
    "    vW,vb1,vV,vb2 = momentum_params\n",
    "    \n",
    "    # Momentum update\n",
    "    vW  = alpha * vW -  lr * (dW + reg*W)\n",
    "    vb1 = alpha * vb1 - lr * (db1 + reg*b1)\n",
    "    vV  = alpha * vV -  lr * (dV + reg*V)\n",
    "    vb2 = alpha * vb2 - lr * (db2 + reg*b2)\n",
    "    momentum_params = [vW,vb1,vV,vb2] \n",
    "    \n",
    "    # Parameter updates\n",
    "    W  = W  + vW\n",
    "    b1 = b1 + vb1\n",
    "    V  = V  + vV\n",
    "    b2 = b2 + vb2\n",
    "    parameters =[W,b1,V,b2]\n",
    "\n",
    "    return parameters, momentum_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#      Building the model     #\n",
    "###############################\n",
    "def run_model(X, Y, cl,\n",
    "              iterations = 1000,\n",
    "              regularization_include = False,\n",
    "              momentum_include = False):    \n",
    "\n",
    "    ###################################\n",
    "    #   Initial values for network    #\n",
    "    ###################################\n",
    "    # Intialize weights\n",
    "    np.random.seed(123)\n",
    "    W = np.random.randn(6).reshape(2,3)\n",
    "    b1 = 0\n",
    "    V = np.random.randn(9).reshape(3,3)\n",
    "    b2 = 0\n",
    "    parameters = [W,b1,V,b2]\n",
    "    \n",
    "    # Hyperparameters \n",
    "    lr = 0.0001 # learning rate\n",
    "    reg = 0.01 * regularization_include\n",
    "    \n",
    "    # Momentum parameters\n",
    "    alpha = 0.9 * momentum_include\n",
    "    vV = 0\n",
    "    vb2 = 0\n",
    "    vW = 0\n",
    "    vb1 = 0\n",
    "    momentum_params = [vW,vb1,vV,vb2]    \n",
    "    \n",
    "    # Place holder for losses\n",
    "    losses = []\n",
    "    errors = []    \n",
    "   \n",
    "    ###################\n",
    "    #   Run the model #\n",
    "    ###################\n",
    "    for i in range(0,iterations):\n",
    "        # -- Forward propoagation --\n",
    "        H,Y_hat = forward(X,parameters)\n",
    "        \n",
    "        # -- Backward propagation --\n",
    "        # Gradient calculation\n",
    "        grads_in = grad(X,H,Y,Y_hat,parameters)\n",
    "        # Parameter update\n",
    "        new_params, new_mom_param = parameter_update(parameters, grads_in, \n",
    "                             momentum_params, alpha = alpha, \n",
    "                             lr = lr, reg = reg)\n",
    "        \n",
    "        # -- Updating values --\n",
    "        H,Y_hat = forward(X,new_params)\n",
    "        parameters = new_params\n",
    "        momentum_params = new_mom_param\n",
    "        \n",
    "        # Prediction and Error rate            \n",
    "        errs_i = error_rate(Y_hat, cl) ; errors.append(errs_i)\n",
    "        loss_i = cost(Y_hat, Y); losses.append(loss_i)\n",
    "        if ((i % 25) == 0):    \n",
    "            print(\n",
    "            '''\n",
    "            ---- Iteration {i} ----\n",
    "            Error rate : {er}\n",
    "            Loss: {loss}\n",
    "            '''.format(i= i, er = errs_i, loss = loss_i))\n",
    "    return {\"errors\":errs_i, \"loss_i\":loss_i, \"parameters\":parameters}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#   Generate some training    #\n",
    "#      data from a GMM        #\n",
    "###############################\n",
    "def gen_gmm_data(n = 999, plot=False):\n",
    "    # Fixing seed for repeatability\n",
    "    np.random.seed(123)\n",
    "    \n",
    "    # Parameters of a normal distribuion\n",
    "    mean_1 = [0, 2] ; mean_2 = [2, -2] ; mean_3 = [-2, -2]\n",
    "    mean = [mean_1, mean_2, mean_3] ; cov = [[1, 0], [0, 1]]  \n",
    "    \n",
    "    # Setting up the class probabilities\n",
    "    n_samples = n\n",
    "    pr_class_1 = pr_class_2 = pr_class_3 = 1/3.0\n",
    "    n_class = (n_samples * np.array([pr_class_1,pr_class_2, pr_class_3])).astype(int)\n",
    "  \n",
    "    # Generate sample data\n",
    "    for i in range(3):\n",
    "        x1,x2 = np.random.multivariate_normal(mean[i], cov, n_class[i]).T\n",
    "        if (i==0):\n",
    "            xs = np.array([x1,x2])\n",
    "            cl = np.array([n_class[i]*[i]])\n",
    "        else: \n",
    "            xs_new = np.array([x1,x2])\n",
    "            cl_new = np.array([n_class[i]*[i]])\n",
    "            xs = np.concatenate((xs, xs_new), axis = 1)\n",
    "            cl = np.concatenate((cl, cl_new), axis = 1)\n",
    "    \n",
    "    # Plot?\n",
    "    if plot:\n",
    "        matplotlib.pyplot.scatter(xs[:1,:],xs[1:,:], c = cl)\n",
    "\n",
    "    # One hot encoding classes\n",
    "    y = pd.Series(cl[0].tolist())\n",
    "    y = pd.get_dummies(y).as_matrix() \n",
    "\n",
    "    # Normalizing data (prevents overflow errors)     \n",
    "    mu = xs.mean(axis = 1)\n",
    "    std = xs.std(axis = 1)\n",
    "    xs = (xs.T - mu) / std\n",
    "    \n",
    "    return xs, y, cl\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#  Generate data for network    #\n",
    "#################################\n",
    "X, Y, cl = gen_gmm_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            ---- Iteration 0 ----\n",
      "            Error rate : 0.36536536536536535\n",
      "            Loss: 815.5770638412325\n",
      "            \n",
      "\n",
      "            ---- Iteration 25 ----\n",
      "            Error rate : 0.15315315315315314\n",
      "            Loss: 567.097113716236\n",
      "            \n",
      "\n",
      "            ---- Iteration 50 ----\n",
      "            Error rate : 0.01901901901901902\n",
      "            Loss: 469.01670271879595\n",
      "            \n",
      "\n",
      "            ---- Iteration 75 ----\n",
      "            Error rate : 0.005005005005005005\n",
      "            Loss: 387.83445409994266\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#   Running Different Models  #\n",
    "###############################\n",
    "vanilla_sgd = run_model(X,Y,cl, iterations = 100,\n",
    "              regularization_include = False,\n",
    "              momentum_include = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
