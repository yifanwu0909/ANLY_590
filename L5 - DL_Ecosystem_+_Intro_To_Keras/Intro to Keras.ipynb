{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to Keras\n",
    "- You'll find that our programming style is very similar to how Keras has implemented networks and training\n",
    "- We'll introduce a number of keys concepts through examples\n",
    "- As the course progresses new methods and api calls will be introduced\n",
    "- We'll review the MNIST dataset and reconstruct our previously built networks using the Keras api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data: Pipeline\n",
    "- Previously we saw how to download data using `keras`\n",
    "- Our data pipeline will follow the steps:\n",
    "    1. Download data\n",
    "    2. Check size of data\n",
    "    3. Convert numeric values to categories using one-hot-encoding\n",
    "    4. Convert data to `float32`\n",
    "    5. Reshape data \n",
    "    6. Rescale the data (so that the values are between 0 and 1)\n",
    "- The aforementioned steps are common in most pre-processing steps before the data can be used\n",
    "- As we move forward in the course a similar approach will be used with other datasets\n",
    "- Note:\n",
    "    - When saving a model's weights, `tf.keras defaults` to the checkpoint format\n",
    "    - Pass `save_format='h5'` to use `HDF5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joshuah/venv3/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n",
      "/home/joshuah/venv3/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/joshuah/venv3/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of training examples: (60000, 28, 28)\n",
      "The number of test examples: (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# --- Load data ---\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# Load pre-shuffled MNIST data into train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# --- Checking size of MNIST Set ---\n",
    "print(\"The number of training examples: \"+str(X_train.shape))\n",
    "print(\"The number of test examples: \"+str(X_test.shape))\n",
    "\n",
    "# --- Y_train to categorical ---\n",
    "# Need to change these from numbers to categories\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)\n",
    "\n",
    "# --- Plotting data ---\n",
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(X_train[0])\n",
    "\n",
    "# --- Converting type and Normalize Values ---\n",
    "n_train, l_train, w_train = X_train.shape\n",
    "n_test, l_test, w_test = X_test.shape\n",
    "X_train = X_train.astype('float32').reshape(n_train, l_train * w_train)\n",
    "X_test = X_test.astype('float32').reshape(n_test, l_test * w_test)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# -- Split Train/Validation set --\n",
    "n_val = 3000\n",
    "X_val = X_train[:n_val,:]; X_train = X_train[n_val:,:]\n",
    "Y_val = Y_train[:n_val,:]; Y_train = Y_train[n_val:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a simple model\n",
    "- Keras has two ways to build models:\n",
    "    1. Sequential - stacked layers\n",
    "    2. Functional - multi input/output, recursive cells\n",
    "- Over the next couple of sessions we'll use the sequential model\n",
    "- The functional approach will come into play in the later parts of the session\n",
    "\n",
    "#### Connecting networks\n",
    "- Previously the input-output matrix shape needed to be specified for each layer \n",
    "- Being explicit about shapes is always a good idea since it forces you to think the architecture of the model\n",
    "- `keras` however provides automatic shape inference \n",
    "- Only the first layer's shape needs to be specified, it can be done in several ways:\n",
    "    - `input_shape` - tuple of integers\n",
    "    - `input_dim` - for 2D layers\n",
    "    - `input_dim`, `input_length` - for 3D temporal layers\n",
    "    - *`batch_size` - may be included as an optional parameter with shape, i.e. `batch_size=32, input_shape=(6,8)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 55,050\n",
      "Trainable params: 55,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# --- Import Librarires ----\n",
    "import tensorflow as tf\n",
    "from keras import Sequential\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# --- Setting up a Sigmoid Sequential Model ---\n",
    "# Initialize model\n",
    "model = Sequential()\n",
    "# Adds a densely-connected layer with 64 units to the model:\n",
    "model.add(Dense(64, input_shape = (784,), activation='sigmoid'))\n",
    "# Add another:\n",
    "model.add(Dense(64, activation='sigmoid'))\n",
    "# Add a softmax layer with 10 output units:\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "# Check model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"264pt\" viewBox=\"0.00 0.00 126.00 264.00\" width=\"126pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 260)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-260 122,-260 122,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 140437751482800 -->\n",
       "<g class=\"node\" id=\"node1\"><title>140437751482800</title>\n",
       "<polygon fill=\"none\" points=\"8,-146.5 8,-182.5 110,-182.5 110,-146.5 8,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"59\" y=\"-160.8\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 140437751504400 -->\n",
       "<g class=\"node\" id=\"node2\"><title>140437751504400</title>\n",
       "<polygon fill=\"none\" points=\"8,-73.5 8,-109.5 110,-109.5 110,-73.5 8,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"59\" y=\"-87.8\">dense_2: Dense</text>\n",
       "</g>\n",
       "<!-- 140437751482800&#45;&gt;140437751504400 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>140437751482800-&gt;140437751504400</title>\n",
       "<path d=\"M59,-146.313C59,-138.289 59,-128.547 59,-119.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"62.5001,-119.529 59,-109.529 55.5001,-119.529 62.5001,-119.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140437751502328 -->\n",
       "<g class=\"node\" id=\"node3\"><title>140437751502328</title>\n",
       "<polygon fill=\"none\" points=\"8,-0.5 8,-36.5 110,-36.5 110,-0.5 8,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"59\" y=\"-14.8\">dense_3: Dense</text>\n",
       "</g>\n",
       "<!-- 140437751504400&#45;&gt;140437751502328 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>140437751504400-&gt;140437751502328</title>\n",
       "<path d=\"M59,-73.3129C59,-65.2895 59,-55.5475 59,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"62.5001,-46.5288 59,-36.5288 55.5001,-46.5289 62.5001,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140437751503168 -->\n",
       "<g class=\"node\" id=\"node4\"><title>140437751503168</title>\n",
       "<polygon fill=\"none\" points=\"0,-219.5 0,-255.5 118,-255.5 118,-219.5 0,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"59\" y=\"-233.8\">140437751503168</text>\n",
       "</g>\n",
       "<!-- 140437751503168&#45;&gt;140437751482800 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>140437751503168-&gt;140437751482800</title>\n",
       "<path d=\"M59,-219.313C59,-211.289 59,-201.547 59,-192.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"62.5001,-192.529 59,-182.529 55.5001,-192.529 62.5001,-192.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Visualizing the model ----\n",
    "# Note: pydot and graphviz required to run command\n",
    "from keras.utils import plot_model\n",
    "# -- Save model to file --\n",
    "plot_model(model, \n",
    "           to_file='/home/joshuah/Desktop/github/guLectureNotes/model.png',\n",
    "           show_shapes=True,\n",
    "           show_layer_names=True,\n",
    "           rankdir='TB') #'TB' = vertical plot, 'LR' = Horizontal plot\n",
    "\n",
    "# -- Libraries for plotting inline --\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the layers\n",
    "- Different layer types are available along with common constructor parameters:\n",
    "- Among the most important is `activation`:\n",
    "    - `sigmoid`,`softmax`, `tanh`, `hard_sigmoid`\n",
    "    - `relu`, `elu`,`selu`\n",
    "    - `linear`\n",
    "    - `softsign`, `softplus`\n",
    "- Advanced activation layers such as `PReLu` and `LeakyReLu` are available through `keras.layers.advanced_activations`\n",
    "- To read more vist the [Advanced Activation Layer](https://keras.io/layers/advanced-activations/) documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 55,050\n",
      "Trainable params: 55,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# --- Setting up a ReLu Sequential Model ---\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(64,activation='relu', input_shape = (784,)))\n",
    "model2.add(Dense(64, activation='relu'))\n",
    "model2.add(Dense(10, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Additional parameters when configuring layers include:    \n",
    "    - `use_bias`: boolean indication if bias should be used\n",
    "    - `kernel_initialize` and `bias_initialize`: initializes weight values `\"Glorot uniform\"` is the default\n",
    "    - `kernel_regularizer` and `bias_regularizer`: regularization schemes applied to different layers\n",
    "    - `kernel_constraint` and `bias_constraint` : construction applied to the kernel weights matrix and bias vector respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 64)                28928     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 42,058\n",
      "Trainable params: 42,058\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# --- Setting up a ReLu Sequential Model with Regularizers ---\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.initializers import constant\n",
    "# -- Model build --\n",
    "model3 = Sequential()\n",
    "# ReLu L1 regularization of factor 0.01 applied to the kernel matrix:\n",
    "model3.add(Dense(64, \n",
    "                 activation = 'relu',\n",
    "                 kernel_regularizer=l1(0.01),\n",
    "                 input_shape = (451,)))\n",
    "\n",
    "# ReLu with L2 regularization of factor 0.01 applied to the bias vector:\n",
    "model3.add(Dense(64, bias_regularizer=l2(0.01)))\n",
    "# Layer with a kernel initialized to a random orthogonal matrix:\n",
    "model3.add(Dense(64, kernel_initializer='orthogonal'))\n",
    "# Layer with a bias vector initialized to 2.0s:\n",
    "model3.add(Dense(64, bias_initializer=constant(2.0)))\n",
    "# Sigmoid output unit\n",
    "model3.add(Dense(10, activation='softmax'))\n",
    "# Model summary\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling a model\n",
    "- Once the model is constructed it needs to be __compiled__\n",
    "- Compiling a model means telling the computer to lazily evaluate:\n",
    "    - The architecture\n",
    "    - Optimization method\n",
    "    - Number of batches\n",
    "    - Metrics to follow\n",
    "- Depending on the type of problem the appropriate parameters will need to be selected\n",
    "- Inputs for the compilation method include:\n",
    "    - `optimizer`: the type of optimization method used during training, common parameters include: `AdamOptimizer`, `RMSPropOptimizer`, `GradientDescentOptimizer`\n",
    "    - ` loss`: function to minimize during optimization\n",
    "        - `mse`: regression\n",
    "        - `categorical_crossentropy`: multiclass-classificaation\n",
    "        - `binary_crossentropy`: vanilla classification\n",
    "    - `mae`: metrics used to monitor training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For a multi-class classification problem\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There might be times when you want to setup a __custom loss metric__:\n",
    "```{python}\n",
    "# --- For custom metrics ---\n",
    "# Import keras backend\n",
    "import keras.backend as K\n",
    "# - Customer metric - \n",
    "def mean_pred(y_true, y_pred):\n",
    "    return K.mean(y_pred)\n",
    "# Compile model with custom metric\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', mean_pred])\n",
    "```\n",
    "\n",
    "- Setting the __optimization hyperparameters __ can also be done before compiling:\n",
    "```\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and evaluating\n",
    "- Use the function `fit` to train the model\n",
    "- Several options exist within the `fit` call:\n",
    "    - `batch_size`: \n",
    "        - The model slices the data into smaller batches and iterates over these batches during training\n",
    "        - This integer specifies the size of each batch\n",
    "        - Recall smaller batch sizes occupy less memory and are closer to online gradient descent\n",
    "    - `epochs` : number of times to iterate over the data\n",
    "    - `shuffle`: shuffle the data\n",
    "    - `validation_data`: validation set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 57000 samples, validate on 3000 samples\n",
      "Epoch 1/3\n",
      "57000/57000 [==============================] - 3s 52us/step - loss: 0.6879 - acc: 0.8398 - val_loss: 0.2929 - val_acc: 0.9180\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 2/3\n",
      "57000/57000 [==============================] - 2s 35us/step - loss: 0.2541 - acc: 0.9263 - val_loss: 0.2203 - val_acc: 0.9357\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 3/3\n",
      "57000/57000 [==============================] - 2s 36us/step - loss: 0.1964 - acc: 0.9422 - val_loss: 0.1836 - val_acc: 0.9477\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Batch size 64: takes 7.150807857513428 seconds to complete\n"
     ]
    }
   ],
   "source": [
    "# Train neural network\n",
    "from time import time\n",
    "start_time = time()\n",
    "history_64 = model.fit(X_train, Y_train,\n",
    "              batch_size=64, # Batch sizes - runtime scales ~linearly\n",
    "              verbose = 1,\n",
    "              epochs=3, \n",
    "              shuffle=False,\n",
    "              validation_data=(X_val, Y_val))\n",
    "time_to_complete = time()- start_time\n",
    "print(\"Batch size 64: takes {} seconds to complete\".format(time_to_complete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbaa897a780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Plotting data/training model ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get loss function\n",
    "loss = history_64.history['loss']\n",
    "val_loss = history_64.history['val_loss']\n",
    "\n",
    "# Setup grid for plotting\n",
    "epochs =range(1, len(loss) + 1)\n",
    "# Plot\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating model performance\n",
    "- Several options exist for evaluate model performance\n",
    "- We can predict the output values using:\n",
    "    - `model.predict` and \n",
    "    - `model.predict_classes` - wrapper for model.predict for classes\n",
    "- Alternatively we can use the `model.evaluate` function and score it's performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 13us/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    }
   ],
   "source": [
    "# --- Evaluation metrics ---\n",
    "import numpy as np\n",
    "# Evaluation metrics\n",
    "score = model.evaluate(X_test, Y_test, batch_size=64)\n",
    "predict_probabilities = model.predict(X_test, batch_size = 64)\n",
    "predict_classes = model.predict_classes(X_test, batch_size = 64)\n",
    "\n",
    "# -- Check to show predict and predict_classes are the same --\n",
    "classes_from_probs =  np.argmax(predict_probabilities, axis = 1)\n",
    "assert(np.alltrue(predict_classes == classes_from_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "On the test set:\n",
      "The mean validation loss is: 0.18258514206707477,\n",
      "The mean accuracy score is: 0.9457\n",
      "The first 5 predict probabilites are: [7 2 1 0 4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "\"\"\"\n",
    "On the test set:\n",
    "The mean validation loss is: {mean_test_loss},\n",
    "The mean accuracy score is: {mean_test_accuracy}\n",
    "The first 5 predict probabilites are: {pred_class}\n",
    "\"\"\".format(mean_test_loss = score[0],\n",
    "           mean_test_accuracy = score[1],\n",
    "          pred_class = predict_classes[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization and parameter tuning\n",
    "- Previously we saw that L1/L2 regularization could easily be incorporated in the activation unit\n",
    "- Other regularizers are added to the network slightly differently\n",
    "- Below we present a network with dropout, custom RMSprop parameters and early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 55,050\n",
      "Trainable params: 55,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbaa895a710>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Setting up a ReLu Sequential Model with Dropout  ---\n",
    "from keras.layers import Dropout\n",
    "# Build model\n",
    "model4 = Sequential()\n",
    "model4.add(Dense(64,activation='relu', input_shape = (784,)))\n",
    "model4.add(Dropout(0.2))\n",
    "model4.add(Dense(64, activation='relu'))\n",
    "model4.add(Dropout(0.2))\n",
    "model4.add(Dense(10, activation='softmax'))\n",
    "model4.summary()\n",
    "\n",
    "# --- Modifying training parameters ---\n",
    "from keras.optimizers import RMSprop\n",
    "opt = RMSprop(lr=0.0001, decay=1e-6)\n",
    "model4.compile(loss='categorical_crossentropy', \n",
    "              optimizer=opt, \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# --- Early stopping ---\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping_monitor = EarlyStopping(patience=2)\n",
    "model4.fit(X_train, Y_train,\n",
    "               batch_size=64, \n",
    "               epochs= 3,\n",
    "               verbose = 0,\n",
    "               validation_data=(X_val, Y_val),\n",
    "               callbacks=[early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving/reloading models\n",
    "- Saving a model in `HDF5` format allows you to load\n",
    "    - The architecture of the model, allowing to re-create the model\n",
    "    - Weights\n",
    "    - Training configuration (loss, optimizer)\n",
    "    - State of the optimizer (you can resume training where you left off)\n",
    "- To save a model in `HDF5` format you will need to install `h5py`\n",
    "- Models can be saved by architecture and weight components in different formats (`YAML`, `json`)\n",
    "- Componentizing this approach offers certain benefits:\n",
    "    - Saving architecture's rather than all the weights frees up space\n",
    "    - Saving only weights makes sense if you already have the model\n",
    "    - Saving models in different formats allows you to port them to other `tensorflow` and `keras` ports/backends such as `tensorflow.js`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --- Setting up directory --\n",
    "import os\n",
    "base_dir = os.getcwd()\n",
    "\n",
    "# --- Saving full models in HDF5 Format ---\n",
    "from keras.models import load_model\n",
    "# Save model in HDF5 fomrat\n",
    "model.save(os.path.join(base_dir,'my_model.h5')) \n",
    "# Deletes existing model\n",
    "del model \n",
    "\n",
    "# --- Load model ---\n",
    "# Returns a compiled model identical to the previous one\n",
    "model = load_model(os.path.join(base_dir,'my_model.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --- Saving only a models architecture ---\n",
    "# Convert model to JSON\n",
    "json_string = model.to_json()\n",
    "\n",
    "# Convert model to YAML\n",
    "yaml_string = model.to_yaml()\n",
    "\n",
    "# --- Reconstructing model from different formats ---\n",
    "from keras.models import model_from_json\n",
    "from keras.models import model_from_yaml\n",
    "# JSON model reload\n",
    "model_json = model_from_json(json_string)\n",
    "# YAML model load\n",
    "model_yaml = model_from_yaml(yaml_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Saving/Loading Model's Weights (only) ---\n",
    "# Vanilla save/load\n",
    "model.save_weights(os.path.join(base_dir,'my_model_weights.h5'))\n",
    "model.load_weights(os.path.join(base_dir,'my_model_weights.h5'))\n",
    "\n",
    "# --- Transfer learning approach to re-load ---\n",
    "# When we load weights into a different \n",
    "# architeture with some common layers\n",
    "model.load_weights(os.path.join(base_dir,'my_model_weights.h5'), \n",
    "                   by_name=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions\n",
    "- There are many functions which we have not covered however this should be enough to get you started\n",
    "- Some other utility functions include `shape`,`configuration` and `weights` which are applied to the model, see below for some examples:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 10)\n",
      "[{'config': {'kernel_regularizer': None, 'use_bias': True, 'kernel_constraint': None, 'activation': 'sigmoid', 'bias_regularizer': None, 'trainable': True, 'kernel_initializer': {'config': {'distribution': 'uniform', 'mode': 'fan_avg', 'seed': None, 'scale': 1.0}, 'class_name': 'VarianceScaling'}, 'activity_regularizer': None, 'bias_constraint': None, 'name': 'dense_1', 'dtype': 'float32', 'batch_input_shape': (None, 784), 'units': 64, 'bias_initializer': {'config': {}, 'class_name': 'Zeros'}}, 'class_name': 'Dense'}, {'config': {'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'activation': 'sigmoid', 'use_bias': True, 'trainable': True, 'kernel_initializer': {'config': {'distribution': 'uniform', 'mode': 'fan_avg', 'seed': None, 'scale': 1.0}, 'class_name': 'VarianceScaling'}, 'activity_regularizer': None, 'bias_constraint': None, 'name': 'dense_2', 'units': 64, 'bias_initializer': {'config': {}, 'class_name': 'Zeros'}}, 'class_name': 'Dense'}, {'config': {'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'activation': 'softmax', 'use_bias': True, 'trainable': True, 'kernel_initializer': {'config': {'distribution': 'uniform', 'mode': 'fan_avg', 'seed': None, 'scale': 1.0}, 'class_name': 'VarianceScaling'}, 'activity_regularizer': None, 'bias_constraint': None, 'name': 'dense_3', 'units': 10, 'bias_initializer': {'config': {}, 'class_name': 'Zeros'}}, 'class_name': 'Dense'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[ 0.01505861,  0.02151298,  0.01216482, ...,  0.02341516,\n",
       "          0.07646369,  0.04311437],\n",
       "        [ 0.00431785, -0.06998525, -0.03343931, ...,  0.02225202,\n",
       "          0.05685513, -0.00061243],\n",
       "        [ 0.07422679,  0.0563629 ,  0.03708457, ...,  0.04175759,\n",
       "         -0.05660856,  0.00496177],\n",
       "        ...,\n",
       "        [ 0.01474265,  0.03832784,  0.07494115, ...,  0.0562676 ,\n",
       "          0.02630088,  0.07625046],\n",
       "        [-0.00797675,  0.0031357 ,  0.05896806, ...,  0.04562616,\n",
       "         -0.07002316, -0.02307474],\n",
       "        [-0.06180535,  0.05619989,  0.00425931, ..., -0.00408291,\n",
       "         -0.01362732,  0.05383138]], dtype=float32),\n",
       " array([-0.07185315, -0.05009798, -0.02087794,  0.03562952,  0.00902029,\n",
       "        -0.00949381,  0.07002638,  0.08355831,  0.03579358, -0.12971082,\n",
       "        -0.13698213,  0.04626996, -0.08944164, -0.05015351,  0.14447387,\n",
       "        -0.00981772, -0.06648774, -0.00129534, -0.12179219, -0.2142365 ,\n",
       "        -0.08730312,  0.07186699,  0.11082583,  0.13890544,  0.01505598,\n",
       "        -0.03164304,  0.21472369, -0.04393366,  0.02848746,  0.08689418,\n",
       "         0.17882653,  0.0623514 , -0.01349953,  0.16603957,  0.00642374,\n",
       "         0.11677251,  0.12624277, -0.12423731,  0.05799632, -0.10206193,\n",
       "         0.04132592,  0.01052886,  0.14727482, -0.00335866,  0.08045447,\n",
       "        -0.07549136,  0.02460519,  0.15116724,  0.01206468, -0.12488707,\n",
       "         0.24204016,  0.08671533,  0.05899089,  0.03814484, -0.09178633,\n",
       "        -0.05502017,  0.05477545, -0.11716684, -0.04334036,  0.00729951,\n",
       "        -0.06630369,  0.0740275 ,  0.09521396, -0.00535981], dtype=float32),\n",
       " array([[-0.54194665, -0.53977   ,  0.40432742, ...,  0.06460738,\n",
       "          0.0340718 ,  0.11810962],\n",
       "        [ 0.01256185,  0.05837477,  0.37275046, ..., -0.3782046 ,\n",
       "          0.17929944, -0.3894613 ],\n",
       "        [ 0.15721196,  0.0564687 ,  0.21532035, ...,  0.4425201 ,\n",
       "         -0.01028986,  0.49724153],\n",
       "        ...,\n",
       "        [-0.12008845, -0.122435  , -0.01221239, ..., -0.4821841 ,\n",
       "          0.49848136, -0.18069409],\n",
       "        [-0.04562577, -0.00895273, -0.36862537, ..., -0.1780558 ,\n",
       "          0.07870965, -0.11953449],\n",
       "        [-0.18175843, -0.29662135, -0.10107408, ..., -0.03903171,\n",
       "         -0.37481463, -0.3385952 ]], dtype=float32),\n",
       " array([ 0.09752251,  0.0563706 ,  0.05447961,  0.07181849,  0.06167331,\n",
       "        -0.00922194,  0.00881361, -0.08866177,  0.02612355, -0.04757936,\n",
       "         0.08763097,  0.0721313 ,  0.00784207, -0.03218774, -0.06371512,\n",
       "         0.06998432,  0.12151269,  0.11390172, -0.02966513, -0.06856442,\n",
       "         0.05937679,  0.09086319,  0.02646548, -0.05674779, -0.00700715,\n",
       "         0.05371512, -0.01321596, -0.02587358, -0.01028969, -0.03104844,\n",
       "        -0.04666967, -0.0478752 ,  0.03397282, -0.07115953, -0.04562065,\n",
       "         0.08948249, -0.06160607, -0.02321623,  0.03621395, -0.0524389 ,\n",
       "        -0.06936773,  0.00115285,  0.07304338, -0.05777591,  0.07555681,\n",
       "         0.00280257, -0.07065158,  0.01846443,  0.06341848,  0.05611705,\n",
       "        -0.01073426,  0.01900975, -0.0236078 , -0.10141124, -0.02513267,\n",
       "         0.09292566,  0.04419835, -0.08655954,  0.05520302,  0.07737788,\n",
       "         0.0875112 ,  0.07224029,  0.02460523,  0.00655798], dtype=float32),\n",
       " array([[-5.81799626e-01, -3.25086951e-01, -7.89192021e-01,\n",
       "          7.09321976e-01, -6.64309323e-01,  2.57166773e-01,\n",
       "         -4.23261285e-01, -8.44708622e-01, -5.93761206e-01,\n",
       "          3.81316990e-01],\n",
       "        [-8.50066662e-01,  4.46170986e-01, -9.10309553e-01,\n",
       "         -3.46967839e-02,  2.31890514e-01,  3.81567568e-01,\n",
       "         -2.95323849e-01,  2.83330560e-01,  3.51412117e-01,\n",
       "         -9.50244814e-02],\n",
       "        [-8.28111112e-01,  4.68741119e-01,  3.34364533e-01,\n",
       "          2.43877456e-01,  4.00422692e-01,  2.17733815e-01,\n",
       "          3.89040858e-01, -9.11323428e-02, -5.40176809e-01,\n",
       "         -7.75634289e-01],\n",
       "        [ 4.87693071e-01, -5.42297006e-01,  3.07538748e-01,\n",
       "          3.54532331e-01, -7.30197847e-01,  4.72059369e-01,\n",
       "         -2.89483368e-01, -1.16509333e-01, -1.17414869e-01,\n",
       "         -3.31995070e-01],\n",
       "        [-1.33046925e-01, -8.20003867e-01,  4.38335359e-01,\n",
       "          3.72430295e-01,  3.43596429e-01, -8.27614293e-02,\n",
       "         -7.75984908e-03, -7.96724141e-01, -8.35257694e-02,\n",
       "          2.90809035e-01],\n",
       "        [ 1.16837986e-01, -4.77163255e-01, -4.28207487e-01,\n",
       "         -4.10877913e-01, -4.37323540e-01,  4.51936930e-01,\n",
       "          5.12794614e-01,  1.58106908e-01, -2.69210428e-01,\n",
       "          1.78800359e-01],\n",
       "        [-4.53996867e-01,  3.05947751e-01, -2.57824272e-01,\n",
       "          3.79966557e-01, -7.22571135e-01, -4.47484285e-01,\n",
       "         -6.98114157e-01,  1.30940259e-01,  1.23403832e-01,\n",
       "         -5.70394874e-01],\n",
       "        [-1.40428901e-01,  2.94071734e-01, -3.73344868e-02,\n",
       "         -3.95998120e-01, -2.48201862e-01,  9.35214385e-02,\n",
       "         -1.73595697e-01,  1.12331146e-02,  4.91718978e-01,\n",
       "          6.02122128e-01],\n",
       "        [ 3.60391289e-01, -2.55343020e-01, -3.68124068e-01,\n",
       "          2.77069032e-01, -7.57579923e-01,  2.90271252e-01,\n",
       "         -4.16070342e-01, -7.41615966e-02,  4.41243589e-01,\n",
       "          4.44771439e-01],\n",
       "        [-8.01853776e-01, -7.96805024e-01, -7.20175058e-02,\n",
       "         -2.28013173e-02,  5.50777197e-01, -8.41343880e-01,\n",
       "         -3.79753858e-01,  4.29488093e-01,  2.27756664e-01,\n",
       "          3.59277517e-01],\n",
       "        [ 2.57988989e-01,  4.00281489e-01,  9.81669873e-03,\n",
       "          3.74081492e-01, -9.59260345e-01, -1.40903085e-01,\n",
       "          3.09043288e-01, -2.53338456e-01,  8.97103339e-04,\n",
       "         -6.17561400e-01],\n",
       "        [ 1.91013828e-01, -7.80259013e-01, -5.52610636e-01,\n",
       "          2.85418063e-01,  3.40315819e-01,  2.19809845e-01,\n",
       "          2.45862961e-01,  3.28843653e-01, -6.80036128e-01,\n",
       "          3.63768846e-01],\n",
       "        [-8.06534767e-01, -9.33391005e-02,  1.73503295e-01,\n",
       "          3.51401269e-02, -1.31901607e-01, -7.64776349e-01,\n",
       "          3.17993402e-01,  2.11968243e-01, -7.77580440e-02,\n",
       "          2.43872389e-01],\n",
       "        [ 2.34324515e-01,  1.56741127e-01,  3.19660932e-01,\n",
       "         -8.26444089e-01,  3.01558733e-01,  5.31850979e-02,\n",
       "          4.49521333e-01,  5.34250200e-01, -5.62955558e-01,\n",
       "         -5.91098964e-01],\n",
       "        [ 3.19606006e-01, -3.60236585e-01,  1.67507395e-01,\n",
       "         -3.51272792e-01, -6.12856984e-01, -4.77443218e-01,\n",
       "          3.87772173e-01,  2.50774860e-01,  3.51292491e-01,\n",
       "         -6.93056703e-01],\n",
       "        [-8.54072511e-01,  4.73196179e-01, -7.45218217e-01,\n",
       "          2.58403122e-01,  8.44126642e-02,  3.91682446e-01,\n",
       "          1.58934877e-03, -7.04413235e-01,  5.34312606e-01,\n",
       "          4.41024899e-02],\n",
       "        [-4.81512636e-01,  2.12290902e-02, -6.43501699e-01,\n",
       "          4.66532588e-01, -7.20139265e-01,  6.77898347e-01,\n",
       "         -5.21516919e-01,  4.50765550e-01, -5.45279682e-01,\n",
       "         -2.70092815e-01],\n",
       "        [ 1.18426736e-02, -2.96935439e-01,  8.89621750e-02,\n",
       "          4.21470881e-01, -1.13125205e-01, -1.97595149e-01,\n",
       "         -1.00982118e+00,  4.71540511e-01, -4.07992095e-01,\n",
       "          7.80560151e-02],\n",
       "        [ 4.27371502e-01,  5.68957388e-01,  2.29096875e-01,\n",
       "         -8.95989686e-02, -6.49267435e-01, -4.50032741e-01,\n",
       "         -2.07925076e-03, -4.98261452e-01,  9.76516008e-02,\n",
       "         -1.35318875e-01],\n",
       "        [-4.39764529e-01,  6.32358789e-01, -5.62815309e-01,\n",
       "         -7.43035614e-01,  4.78858471e-01,  2.14195117e-01,\n",
       "         -2.69650996e-01, -2.66627967e-01, -6.72039151e-01,\n",
       "          3.91568452e-01],\n",
       "        [ 1.85971148e-02,  4.19605702e-01,  2.33467266e-01,\n",
       "         -4.57995608e-02, -2.81147152e-01,  4.27913904e-01,\n",
       "         -2.73097187e-01,  6.69270337e-01, -3.69317412e-01,\n",
       "         -7.57793069e-01],\n",
       "        [-6.19812906e-01,  1.65124625e-01,  1.53783739e-01,\n",
       "          1.00254128e-02,  3.40091176e-02,  3.86274517e-01,\n",
       "          3.87684137e-01, -7.37785578e-01, -3.93596515e-02,\n",
       "          1.89096004e-01],\n",
       "        [-5.80753446e-01,  5.75600564e-01,  3.75459194e-01,\n",
       "          3.42657954e-01,  5.42193130e-02, -6.80728078e-01,\n",
       "         -8.09017718e-01,  5.71027219e-01, -2.23851606e-01,\n",
       "         -4.59458292e-01],\n",
       "        [ 6.04367197e-01, -9.74417269e-01, -4.49703306e-01,\n",
       "         -3.32830357e-03,  8.72600526e-02, -3.33104014e-01,\n",
       "          5.40024221e-01,  1.40153253e-02, -6.92934394e-01,\n",
       "         -3.63045037e-01],\n",
       "        [-2.97048181e-01, -7.24693000e-01, -3.27857696e-02,\n",
       "          5.83159253e-02,  2.74478614e-01,  4.46685225e-01,\n",
       "         -2.75108099e-01, -2.72831708e-01,  5.01029074e-01,\n",
       "         -4.50140893e-01],\n",
       "        [ 6.11396968e-01, -3.97305429e-01, -3.57692808e-01,\n",
       "          4.67597604e-01, -3.84730309e-01,  6.70959130e-02,\n",
       "         -7.97161758e-01,  2.76426047e-01, -6.16546333e-01,\n",
       "          2.42757455e-01],\n",
       "        [-1.00011277e+00,  9.48871672e-02, -6.36043012e-01,\n",
       "         -2.11396635e-01,  4.32596684e-01, -1.75304394e-02,\n",
       "         -8.97339523e-01,  1.91930458e-01,  3.21716666e-01,\n",
       "          2.72986770e-01],\n",
       "        [ 2.28053972e-01,  2.80728042e-02, -7.76234031e-01,\n",
       "         -7.43067563e-01, -9.28933173e-02, -5.89544959e-02,\n",
       "          5.64615764e-02, -1.84902400e-02, -4.63171512e-01,\n",
       "          1.39458805e-01],\n",
       "        [ 5.05023956e-01, -1.41092733e-01,  3.79226476e-01,\n",
       "         -3.44334155e-01, -3.87560964e-01,  7.97374472e-02,\n",
       "          4.69348937e-01, -1.83107436e-01,  2.57787615e-01,\n",
       "         -4.84440029e-01],\n",
       "        [ 5.15857935e-01, -8.36628795e-01,  4.02712405e-01,\n",
       "         -1.67014390e-01, -1.78598583e-01, -4.64543343e-01,\n",
       "         -6.72972202e-01, -2.41320685e-01,  3.20927918e-01,\n",
       "         -3.28796327e-01],\n",
       "        [ 7.60666579e-02,  5.01755655e-01, -7.48670936e-01,\n",
       "         -6.59218252e-01,  1.08077981e-01, -2.46253401e-01,\n",
       "          3.22598398e-01,  1.85894817e-01, -4.96097147e-01,\n",
       "          3.70659202e-01],\n",
       "        [-4.26412135e-01, -7.42254198e-01,  2.69367725e-01,\n",
       "         -2.64863651e-02,  2.60197580e-01, -1.69652384e-02,\n",
       "          4.50132430e-01, -7.26388395e-01, -2.57364720e-01,\n",
       "          3.63314450e-01],\n",
       "        [ 2.85964459e-01, -8.85049045e-01, -4.75559652e-01,\n",
       "         -3.61923963e-01, -2.00907320e-01,  1.07015327e-01,\n",
       "         -3.86302561e-01,  1.21099375e-01,  2.46007755e-01,\n",
       "          4.88963395e-01],\n",
       "        [-4.49689291e-02, -2.02137128e-01,  1.72018677e-01,\n",
       "         -7.94559121e-01,  2.80712903e-01,  7.81906471e-02,\n",
       "          4.32461292e-01, -5.42536378e-01,  2.33572215e-01,\n",
       "          1.38187528e-01],\n",
       "        [-9.02287722e-01,  5.08212805e-01, -3.39464009e-01,\n",
       "         -7.79628515e-01,  4.41800147e-01,  2.38083467e-01,\n",
       "         -5.92510879e-01,  2.81878531e-01,  1.55276075e-01,\n",
       "         -8.98432955e-02],\n",
       "        [ 1.37138590e-01,  2.57805377e-01, -8.14875007e-01,\n",
       "          3.84425342e-01, -2.14080974e-01,  5.45093656e-01,\n",
       "         -3.12182933e-01, -7.39696980e-01,  4.72305417e-01,\n",
       "         -6.21218622e-01],\n",
       "        [-6.84678778e-02, -4.67404664e-01,  1.41038492e-01,\n",
       "         -3.19794357e-01,  3.63866761e-02,  3.07803541e-01,\n",
       "          2.00643018e-01, -1.02334714e+00,  4.25567567e-01,\n",
       "         -2.65290022e-01],\n",
       "        [ 2.45330706e-01, -7.85067856e-01,  4.99106705e-01,\n",
       "         -7.19711483e-01,  6.21803999e-02, -7.71955729e-01,\n",
       "         -1.07251577e-01,  1.48673549e-01,  3.30343917e-02,\n",
       "          9.33415517e-02],\n",
       "        [-3.83688301e-01, -4.42160904e-01,  3.70169818e-01,\n",
       "          1.51471451e-01,  5.79152167e-01,  1.56939387e-01,\n",
       "          4.68151510e-01, -6.40522063e-01,  1.91653833e-01,\n",
       "         -5.35684884e-01],\n",
       "        [ 2.34748185e-01,  3.38550836e-01,  1.32427067e-01,\n",
       "         -5.86848199e-01,  1.98322386e-01,  4.24606800e-01,\n",
       "          1.73395604e-01, -7.79813170e-01,  2.35462457e-01,\n",
       "         -2.67785251e-01],\n",
       "        [-8.13476980e-01,  5.34039021e-01, -1.32969201e-01,\n",
       "         -3.18181962e-01,  5.06786466e-01, -7.98289537e-01,\n",
       "          3.61227579e-02, -2.54564047e-01, -4.39597458e-01,\n",
       "          4.82937455e-01],\n",
       "        [-6.62789881e-01,  2.35995978e-01, -5.45358360e-02,\n",
       "          3.44641685e-01, -2.78611332e-01, -9.52149212e-01,\n",
       "          2.64459640e-01,  2.68635213e-01,  1.27859354e-01,\n",
       "          3.56144339e-01],\n",
       "        [-7.84865558e-01,  2.57069439e-01,  5.63680768e-01,\n",
       "          4.31618899e-01, -5.87482691e-01, -6.78099275e-01,\n",
       "         -6.35121346e-01, -3.04434597e-01,  1.52207837e-01,\n",
       "         -3.21305394e-01],\n",
       "        [ 4.68434811e-01,  2.62660086e-01, -1.13911435e-01,\n",
       "         -6.22316122e-01, -3.88648540e-01, -2.99569279e-01,\n",
       "         -9.31449421e-03,  1.33450702e-01,  7.89828151e-02,\n",
       "          3.95146847e-01],\n",
       "        [-7.03329325e-01,  4.32610571e-01,  8.99198502e-02,\n",
       "          2.76423484e-01, -5.73375642e-01,  1.28618285e-01,\n",
       "         -4.22446787e-01, -1.94965556e-01,  4.98886913e-01,\n",
       "          2.69069672e-01],\n",
       "        [ 5.96454799e-01, -2.69800395e-01,  5.41140079e-01,\n",
       "         -2.83437669e-02, -6.36617899e-01, -4.26284790e-01,\n",
       "         -6.62969351e-01, -2.11415812e-01,  2.04401091e-01,\n",
       "         -7.56521940e-01],\n",
       "        [-1.70052558e-01,  3.44044447e-01,  3.24061185e-01,\n",
       "         -5.30146360e-01,  6.64586648e-02, -5.86028457e-01,\n",
       "          3.85092050e-01, -1.11837715e-01, -1.50363207e-01,\n",
       "         -5.00365257e-01],\n",
       "        [ 5.82608879e-01, -3.25002015e-01, -2.84350570e-02,\n",
       "         -1.46694556e-02, -6.93133235e-01, -3.68132740e-01,\n",
       "         -6.73130035e-01,  4.36745644e-01,  1.62170514e-01,\n",
       "          6.31527722e-01],\n",
       "        [ 4.55589354e-01,  4.30897593e-01,  4.19070035e-01,\n",
       "          1.43798620e-01,  1.44533023e-01,  2.83164322e-01,\n",
       "         -8.38362932e-01,  6.12218320e-01, -9.10439968e-01,\n",
       "         -4.57637787e-01],\n",
       "        [ 4.90054280e-01,  2.86689550e-01, -4.64614891e-02,\n",
       "         -2.38107875e-01, -8.87302995e-01,  3.67074519e-01,\n",
       "          2.59810448e-01, -1.00749671e+00,  2.59123862e-01,\n",
       "         -9.31904197e-01],\n",
       "        [-5.41588128e-01, -1.96870103e-01, -3.65749836e-01,\n",
       "          1.60194904e-01,  3.40388119e-01,  2.94822510e-02,\n",
       "         -6.04757249e-01, -9.97347713e-01,  6.06713414e-01,\n",
       "          4.85483706e-01],\n",
       "        [-3.84024382e-01, -6.76476479e-01, -8.05635750e-02,\n",
       "          2.68870324e-01,  4.91403282e-01,  3.20964903e-01,\n",
       "         -4.72191542e-01,  5.17077409e-02,  2.45890141e-01,\n",
       "         -3.89516920e-01],\n",
       "        [ 2.89124191e-01, -5.07717490e-01, -3.90157670e-01,\n",
       "         -3.73155922e-01,  4.10761297e-01,  5.54833293e-01,\n",
       "          5.41308999e-01, -3.73204470e-01, -6.94168746e-01,\n",
       "          1.51447117e-01],\n",
       "        [ 4.23194528e-01, -1.01069629e+00, -1.57863908e-02,\n",
       "         -5.24798632e-01,  3.36599022e-01,  2.24579662e-01,\n",
       "          3.25544477e-01,  1.32457510e-01,  1.51225775e-02,\n",
       "          1.90063581e-01],\n",
       "        [ 2.39046551e-02,  3.61645520e-01,  2.49821246e-01,\n",
       "         -2.46867880e-01,  5.29592156e-01, -1.86535940e-01,\n",
       "         -1.32525444e-01, -1.23294927e-01, -5.11965156e-01,\n",
       "         -6.74394906e-01],\n",
       "        [-5.25768995e-01, -2.07082033e-01, -3.71662498e-01,\n",
       "          5.66936553e-01,  2.47409314e-01, -3.88190061e-01,\n",
       "         -9.68277693e-01,  1.31646127e-01, -6.44994974e-01,\n",
       "         -1.18723348e-01],\n",
       "        [ 5.01178920e-01, -3.45938027e-01,  4.95594919e-01,\n",
       "          3.56435001e-01, -5.76261520e-01, -2.64539629e-01,\n",
       "          3.41048241e-01,  6.17295086e-01, -8.10448945e-01,\n",
       "         -4.78643596e-01],\n",
       "        [-1.45491675e-01, -4.78498250e-01, -8.53931159e-02,\n",
       "         -5.31282127e-01, -1.66870564e-01, -5.39491236e-01,\n",
       "         -8.82975578e-01,  3.44853163e-01,  3.90128702e-01,\n",
       "          3.01627070e-01],\n",
       "        [-6.39600381e-02,  2.70371616e-01,  3.17179829e-01,\n",
       "          3.22314769e-01, -7.22966373e-01,  3.37924331e-01,\n",
       "          6.01524532e-01, -9.13818538e-01, -4.43707317e-01,\n",
       "         -4.55430955e-01],\n",
       "        [ 3.89446944e-01, -5.26627362e-01, -7.07182765e-01,\n",
       "          2.65789270e-01,  4.11397636e-01,  4.31595355e-01,\n",
       "         -4.04448926e-01,  2.82232523e-01, -1.89203411e-01,\n",
       "          1.60371542e-01],\n",
       "        [-5.98444879e-01, -2.57974923e-01, -4.73685831e-01,\n",
       "          4.11510795e-01, -6.35959744e-01,  1.94475070e-01,\n",
       "          9.35886577e-02, -1.07616723e+00,  2.44889095e-01,\n",
       "         -4.07400012e-01],\n",
       "        [-6.26140416e-01,  8.01121891e-02,  9.30207446e-02,\n",
       "          2.45644301e-01, -5.49463987e-01,  2.96876878e-01,\n",
       "         -1.13852501e+00,  4.19958830e-01,  3.16334926e-02,\n",
       "          5.12777328e-01],\n",
       "        [-7.93518305e-01,  2.60202855e-01, -7.08476007e-02,\n",
       "          1.09107018e-01,  6.53310716e-01, -2.96709865e-01,\n",
       "          5.24962962e-01, -5.27694643e-01, -2.79247910e-01,\n",
       "         -6.59172237e-01],\n",
       "        [-6.34401619e-01,  2.59784043e-01,  4.58345383e-01,\n",
       "          1.79584384e-01,  3.50826383e-02, -9.41471815e-01,\n",
       "         -8.18793178e-01,  4.64660048e-01,  1.73385575e-01,\n",
       "          5.08905053e-01]], dtype=float32),\n",
       " array([-0.00077297,  0.02825757, -0.03847226, -0.09026763,  0.01408697,\n",
       "        -0.03214766,  0.01975505, -0.00791471, -0.06144809,  0.02137348],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Utility functions ---\n",
    "print(model.output_shape)\n",
    "print(model.get_config())\n",
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises:\n",
    "1. Build and fit a `relu` network\n",
    "2. Modify the batch size in powers of 2 and compare the runtimes and performance\n",
    "3. Include a regularizer in different parts of your network\n",
    "4. Compare `adam` and `rmsprop` optimizer performance\n",
    "5. Using `adam` look up the api to define it's parameters and modify them"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
