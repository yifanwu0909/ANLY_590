{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structure of notebook\n",
    "- This notebook will serve as an introduction to the word embedding process in Keras.\n",
    "- Here we introduce:\n",
    "    1. Tokenization\n",
    "    2. Embedding\n",
    "- The following notebook will show you how to:\n",
    "    1. Load a trained vector model\n",
    "    2. Use embedding for building a simple model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One hot encoding and text tokenization\n",
    "Here we are going to tokenize our text and one-hot encode the words using keras's in-built tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "samples = ['The quick brown fox.', 'Jumped over the lazy fox.']\n",
    "# Creates a tokenizer, configured to only take into account the 1000 most common words\n",
    "# Note that we only have 7\n",
    "tokenizer = Tokenizer(num_words = 1000)\n",
    "# Building the word index\n",
    "tokenizer.fit_on_texts(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 3, 4, 2], [5, 6, 1, 7, 2]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turns strings into lists of integer indices\n",
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9   ...   990  991  992  993  \\\n",
       "0  0.0  1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0  0.0   \n",
       "1  0.0  1.0  1.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0 ...   0.0  0.0  0.0  0.0   \n",
       "\n",
       "   994  995  996  997  998  999  \n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[2 rows x 1000 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turns string into binary vector of of dim 1000 (based on word limit above)\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n",
    "pd.DataFrame(one_hot_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 unique tokens\n",
      "The dictionary mapping of tokens is\n",
      " {'over': 6, 'brown': 4, 'the': 1, 'quick': 3, 'fox': 2, 'jumped': 5, 'lazy': 7}\n"
     ]
    }
   ],
   "source": [
    "# Dictionary mapping of words to one-hot-encoded index value\n",
    "word_index = tokenizer.word_index\n",
    "print('Found {} unique tokens'.format(len(word_index)))\n",
    "print('The dictionary mapping of tokens is\\n {}'.format(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashing trick\n",
    "- For very large vacabularies one-hot-encoding will not work. \n",
    "- Rather we use one-hot hashing which uses a light-weight hashing function to hash words into vectors of fixed size (rather than maintaing an index).\n",
    "\n",
    "- Advantages:\n",
    "    - Do not need to maintain word index\n",
    "    - Saves memory\n",
    "    - Allows online encoding of data (can generate token vectors on the fly before all data has been seen)\n",
    "\n",
    "- Disadvantages:\n",
    "    - Hash-collisions (occurs when 2 words occupt the same hash)\n",
    "\n",
    "- Practicalities:\n",
    "    - If the dimensionality of the hash-space is large then hash-collisions are unlikely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example of one-hot hashing\n",
    "samples = ['The quick brown fox.', 'Jumped over the lazy fox.']\n",
    "dimensionality = 10\n",
    "max_length = 10\n",
    "\n",
    "# Pre-allocation\n",
    "results = np.zeros((len(samples), max_length, dimensionality))\n",
    "\n",
    "# Hashing function \n",
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        # Hashes word into a random integer index between 0 and 1000\n",
    "        index = abs(hash(word)) % dimensionality\n",
    "        results[i, j, index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 arrays with hashes\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up an embedding layer.\n",
    "- The embedding layer takes in a 2d tensor of integers of the dimension `(samples, sequence_length)` \n",
    "    - It accept batches of size `samples` and \n",
    "    - The string will need to be either 0 padded or truncated to reach the `sequence_length`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "# Embedding takes two values\n",
    "# Embedding:(n,d) = (max number of tokens, embedding dimension)\n",
    "embedding_layer = Embedding(1000, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here we'll use the imdb dataset\n",
    "    - The `x` values are tokenized values of words\n",
    "    - The `y` values are the sentiment score\n",
    "- We'll restrict our voacbulary to the 10000 most popular words with and cut-off reviews after 20 words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Loading librarires ---\n",
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n",
    "\n",
    "# --- Setting up constants ---\n",
    "# Number of words as features, we keep only the top most-common words\n",
    "max_features = 1000\n",
    "# Max number of words in a review (truncate the rest)\n",
    "maxlen = 20\n",
    "# --- Reading in in data ---\n",
    "# Loads the data as lists of integers\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tokenized vector for the first review:\n",
      "     0\n",
      "0    1\n",
      "1  194\n",
      "2    2\n",
      "3  194\n",
      "4    2\n",
      "5   78\n",
      "6  228\n",
      "7    5\n",
      "8    6\n",
      "9    2\n",
      "The sentiment for the first review   0\n",
      "0  0\n"
     ]
    }
   ],
   "source": [
    "# What do our x_train, y_train look like?\n",
    "print(\"The tokenized vector for the first review:\")\n",
    "print(pd.DataFrame(x_train[1]).head(10))\n",
    "print(\"The sentiment for the first review\" + str(pd.DataFrame(y_train[[1]])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --- Preprocessing data to pad/truncate sequences ---\n",
    "# Turns the lists of integers into a 2d integer tensor of shape (samples, maxlen)\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen = maxlen)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen = maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 20)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 23,   4,   2,  15,  16,   4,   2,   5,  28,   6,  52, 154, 462,\n",
       "        33,  89,  78, 285,  16, 145,  95], dtype=int32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training a model and embedding layer\n",
    "- Let's now train the classifier and the weights from the embedding layer\n",
    "- Note:\n",
    "    - The Embedding layer weights, like all other weights in the network will be trained (e.g. with stochastic gradient descent)\n",
    "    - Word embeddings can be pretrained with w2v and use them as initial weights for the Embedding layer\n",
    "    - You can then make the weights static or trainable, depending on your preference\n",
    "\n",
    "- The model we will train will be a single dense layer on top for classification \n",
    "    - This is equivalent to a simple logisitic regression\n",
    "    - We do not consider inter-word relationships\n",
    "    - Recurrent nets take into account word relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 20, 8)             80000     \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 161       \n",
      "=================================================================\n",
      "Total params: 80,161\n",
      "Trainable params: 80,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense\n",
    "\n",
    "# Setting up keras model\n",
    "model = Sequential()\n",
    "\n",
    "# Create embedding layer as input\n",
    "# 1000 - number of words\n",
    "# 8 - embedding dimension\n",
    "# input_length = length of phrase\n",
    "model.add(Embedding(10000,8, input_length = maxlen))\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add sigmoid layer\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(optimizer = 'rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 2s - loss: 0.5037 - acc: 0.7503 - val_loss: 0.5271 - val_acc: 0.7272\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 2s - loss: 0.4875 - acc: 0.7607 - val_loss: 0.5245 - val_acc: 0.7350\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 2s - loss: 0.4774 - acc: 0.7682 - val_loss: 0.5263 - val_acc: 0.7344\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 2s - loss: 0.4703 - acc: 0.7703 - val_loss: 0.5270 - val_acc: 0.7328\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 2s - loss: 0.4635 - acc: 0.7749 - val_loss: 0.5290 - val_acc: 0.7336\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1208b8d50>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit(x_train, y_train,\n",
    "                   epochs = 5,\n",
    "                   batch_size = 32,\n",
    "                   validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Our validation accuracy is not so bad around ~75%\n",
    "- Considering that there is a 50% chance of being correct that's a 50% boost in accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in this example, we trained our own word embeddings for this specific classification task. The unsupervised way of doing word embeddings is to use skip-gram or CBOW approach. Then, the learned embeddings can be useful for all kinds of different classification tasks. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "4px",
    "width": "253px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
